# harmonypy - A data alignment algorithm.
# Copyright (C) 2018  Ilya Korsunsky
#               2019  Kamil Slowikowski <kslowikowski@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import pandas as pd
import numpy as np
import torch
from sklearn.cluster import KMeans
import logging

# create logger
logger = logging.getLogger('harmonypy')
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    logger.addHandler(ch)


def get_device(device=None):
    """Get the appropriate device for PyTorch operations."""
    if device is not None:
        return torch.device(device)
    
    # Check for available accelerators
    if torch.cuda.is_available():
        return torch.device('cuda')
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        return torch.device('mps')
    else:
        return torch.device('cpu')


def run_harmony(
    data_mat: np.ndarray,
    meta_data: pd.DataFrame,
    vars_use,
    theta=None,
    lamb=None,
    sigma=0.1, 
    nclust=None,
    tau=0,
    block_size=0.05, 
    max_iter_harmony=10,
    max_iter_kmeans=20,
    epsilon_cluster=1e-5,
    epsilon_harmony=1e-4, 
    alpha=0.2,
    verbose=True,
    random_state=0,
    device=None
):
    """Run Harmony batch effect correction.
    
    This is a PyTorch implementation matching the R package formulas.
    Supports CPU and GPU (CUDA, MPS) acceleration.
    
    Parameters
    ----------
    data_mat : np.ndarray
        PCA embedding matrix (cells x PCs or PCs x cells)
    meta_data : pd.DataFrame
        Metadata with batch variables
    vars_use : str or list
        Column name(s) in meta_data to use for batch correction
    theta : float or list, optional
        Diversity penalty parameter(s). Default is 2 for each batch.
    lamb : float or list, optional
        Ridge regression penalty. Default is 1 for each batch.
        If -1, lambda is estimated automatically (matches R package).
    sigma : float, optional
        Kernel bandwidth for soft clustering. Default is 0.1.
    nclust : int, optional
        Number of clusters. Default is min(N/30, 100).
    tau : float, optional
        Protection against overcorrection. Default is 0.
    block_size : float, optional
        Proportion of cells to update in each block. Default is 0.05.
    max_iter_harmony : int, optional
        Maximum Harmony iterations. Default is 10.
    max_iter_kmeans : int, optional
        Maximum k-means iterations per Harmony iteration. Default is 20.
    epsilon_cluster : float, optional
        K-means convergence threshold. Default is 1e-5.
    epsilon_harmony : float, optional
        Harmony convergence threshold. Default is 1e-4.
    alpha : float, optional
        Alpha parameter for lambda estimation (when lamb=-1). Default is 0.2.
    verbose : bool, optional
        Print progress messages. Default is True.
    random_state : int, optional
        Random seed for reproducibility. Default is 0.
    device : str, optional
        Device to use ('cpu', 'cuda', 'mps'). Default is auto-detect.
        
    Returns
    -------
    Harmony
        Harmony object with corrected data in Z_corr attribute.
    """
    N = meta_data.shape[0]
    if data_mat.shape[1] != N:
        data_mat = data_mat.T

    assert data_mat.shape[1] == N, \
       "data_mat and meta_data do not have the same number of cells" 

    if nclust is None:
        nclust = int(min(round(N / 30.0), 100))

    if isinstance(sigma, float) and nclust > 1:
        sigma = np.repeat(sigma, nclust)

    if isinstance(vars_use, str):
        vars_use = [vars_use]

    # Create batch indicator matrix (one-hot encoded)
    phi = pd.get_dummies(meta_data[vars_use]).to_numpy().T.astype(np.float32)
    phi_n = meta_data[vars_use].describe().loc['unique'].to_numpy().astype(int)

    # Theta handling - default is 2 (matches R package)
    if theta is None:
        theta = np.repeat([2] * len(phi_n), phi_n).astype(np.float32)
    elif isinstance(theta, (float, int)):
        theta = np.repeat([theta] * len(phi_n), phi_n).astype(np.float32)
    elif len(theta) == len(phi_n):
        theta = np.repeat([theta], phi_n).astype(np.float32)
    else:
        theta = np.asarray(theta, dtype=np.float32)

    assert len(theta) == np.sum(phi_n), \
        "each batch variable must have a theta"

    # Lambda handling (matches R package)
    lambda_estimation = False
    if lamb is None:
        lamb = np.repeat([1] * len(phi_n), phi_n).astype(np.float32)
        lamb = np.insert(lamb, 0, 0).astype(np.float32)
    elif lamb == -1:
        lambda_estimation = True
        lamb = np.zeros(1, dtype=np.float32)
    elif isinstance(lamb, (float, int)):
        lamb = np.repeat([lamb] * len(phi_n), phi_n).astype(np.float32)
        lamb = np.insert(lamb, 0, 0).astype(np.float32)
    elif len(lamb) == len(phi_n):
        lamb = np.repeat([lamb], phi_n).astype(np.float32)
        lamb = np.insert(lamb, 0, 0).astype(np.float32)
    else:
        lamb = np.asarray(lamb, dtype=np.float32)
        if len(lamb) == np.sum(phi_n):
            lamb = np.insert(lamb, 0, 0).astype(np.float32)

    # Number of items in each category
    N_b = phi.sum(axis=1)
    Pr_b = (N_b / N).astype(np.float32)

    if tau > 0:
        theta = theta * (1 - np.exp(-(N_b / (nclust * tau)) ** 2))

    # Get device
    device_obj = get_device(device)

    if verbose:
        logger.info(f"Running Harmony (PyTorch on {device_obj})")
        logger.info("  Parameters:")
        logger.info(f"    max_iter_harmony: {max_iter_harmony}")
        logger.info(f"    max_iter_kmeans: {max_iter_kmeans}")
        logger.info(f"    epsilon_cluster: {epsilon_cluster}")
        logger.info(f"    epsilon_harmony: {epsilon_harmony}")
        logger.info(f"    nclust: {nclust}")
        logger.info(f"    block_size: {block_size}")
        if lambda_estimation:
            logger.info(f"    lamb: dynamic (alpha={alpha})")
        else:
            logger.info(f"    lamb: {lamb[1:]}")
        logger.info(f"    theta: {theta}")
        logger.info(f"    sigma: {sigma[:5]}..." if len(sigma) > 5 else f"    sigma: {sigma}")
        logger.info(f"    verbose: {verbose}")
        logger.info(f"    random_state: {random_state}")
        logger.info(f"  Data: {data_mat.shape[0]} PCs Ã— {N} cells")
        logger.info(f"  Batch variables: {vars_use}")

    # Set random seeds
    np.random.seed(random_state)
    torch.manual_seed(random_state)

    # Ensure data_mat is a proper numpy array
    if hasattr(data_mat, 'values'):
        data_mat = data_mat.values
    data_mat = np.asarray(data_mat, dtype=np.float32)
    
    ho = Harmony(
        data_mat, phi, Pr_b, sigma.astype(np.float32), 
        theta, lamb, alpha, lambda_estimation,
        max_iter_harmony, max_iter_kmeans,
        epsilon_cluster, epsilon_harmony, nclust, block_size, verbose,
        random_state, device_obj
    )

    return ho


class Harmony:
    """Harmony class for batch effect correction using PyTorch.
    
    Supports CPU and GPU acceleration.
    """
    
    def __init__(
            self, Z, Phi, Pr_b, sigma, theta, lamb, alpha, lambda_estimation,
            max_iter_harmony, max_iter_kmeans, 
            epsilon_kmeans, epsilon_harmony, K, block_size, verbose,
            random_state, device
    ):
        self.device = device
        
        # Convert to PyTorch tensors on device
        # Store as _Z_corr internally, expose Z_corr as property for backward compatibility
        self._Z_corr = torch.tensor(Z, dtype=torch.float32, device=device)
        self.Z_orig = torch.tensor(Z, dtype=torch.float32, device=device)
        
        # Simple L2 normalization
        self.Z_cos = self.Z_orig / torch.linalg.norm(self.Z_orig, ord=2, dim=0)
        
        # Batch indicators
        self.Phi = torch.tensor(Phi, dtype=torch.float32, device=device)
        self.Pr_b = torch.tensor(Pr_b, dtype=torch.float32, device=device)
        
        self.N = self._Z_corr.shape[1]
        self.B = Phi.shape[0]
        self.d = self._Z_corr.shape[0]
        
        # Build batch index for fast ridge correction
        self.batch_index = []
        for b in range(self.B):
            idx = torch.where(self.Phi[b, :] > 0)[0]
            self.batch_index.append(idx)
        
        # Create Phi_moe with intercept
        ones = torch.ones(1, self.N, dtype=torch.float32, device=device)
        self.Phi_moe = torch.cat([ones, self.Phi], dim=0)
        
        self.window_size = 3
        self.epsilon_kmeans = epsilon_kmeans
        self.epsilon_harmony = epsilon_harmony

        self.lamb = torch.tensor(lamb, dtype=torch.float32, device=device)
        self.alpha = alpha
        self.lambda_estimation = lambda_estimation
        self.sigma = torch.tensor(sigma, dtype=torch.float32, device=device)
        self.block_size = block_size
        self.K = K
        self.max_iter_harmony = max_iter_harmony
        self.max_iter_kmeans = max_iter_kmeans
        self.verbose = verbose
        self.theta = torch.tensor(theta, dtype=torch.float32, device=device)

        self.objective_harmony = []
        self.objective_kmeans = []
        self.objective_kmeans_dist = []
        self.objective_kmeans_entropy = []
        self.objective_kmeans_cross = []
        self.kmeans_rounds = []

        self.allocate_buffers()
        self.init_cluster(random_state)
        self.harmonize(self.max_iter_harmony, self.verbose)

    @property
    def Z_corr(self):
        """Return corrected data as NumPy array (backward compatible)."""
        return self._Z_corr.cpu().numpy()

    def result(self):
        """Return corrected data as NumPy array."""
        return self._Z_corr.cpu().numpy()

    def allocate_buffers(self):
        self._scale_dist = torch.zeros((self.K, self.N), dtype=torch.float32, device=self.device)
        self.dist_mat = torch.zeros((self.K, self.N), dtype=torch.float32, device=self.device)
        self.O = torch.zeros((self.K, self.B), dtype=torch.float32, device=self.device)
        self.E = torch.zeros((self.K, self.B), dtype=torch.float32, device=self.device)
        self.W = torch.zeros((self.B + 1, self.d), dtype=torch.float32, device=self.device)
        self.R = torch.zeros((self.K, self.N), dtype=torch.float32, device=self.device)
        self.Y = torch.zeros((self.d, self.K), dtype=torch.float32, device=self.device)

    def init_cluster(self, random_state):
        logger.info("Computing initial centroids with sklearn.KMeans...")
        # KMeans needs CPU numpy array
        Z_cos_np = self.Z_cos.cpu().numpy()
        model = KMeans(n_clusters=self.K, init='k-means++',
                       n_init=1, max_iter=25, random_state=random_state)
        model.fit(Z_cos_np.T)
        self.Y = torch.tensor(model.cluster_centers_.T, dtype=torch.float32, device=self.device)
        logger.info("KMeans initialization complete.")
        
        # Normalize centroids
        self.Y = self.Y / torch.linalg.norm(self.Y, ord=2, dim=0)
        
        # Compute distance matrix: dist = 2 * (1 - Y.T @ Z_cos)
        self.dist_mat = 2 * (1 - self.Y.T @ self.Z_cos)
        
        # Compute R
        self.R = -self.dist_mat / self.sigma[:, None]
        self.R = torch.exp(self.R)
        self.R = self.R / self.R.sum(dim=0)
        
        # Batch diversity statistics
        self.E = torch.outer(self.R.sum(dim=1), self.Pr_b)
        self.O = self.R @ self.Phi.T
        
        self.compute_objective()
        self.objective_harmony.append(self.objective_kmeans[-1])

    def compute_objective(self):
        # Normalization constant
        norm_const = 2000.0 / self.N
        
        # K-means error
        kmeans_error = torch.sum(self.R * self.dist_mat).item()
        
        # Entropy
        _entropy = torch.sum(safe_entropy_torch(self.R) * self.sigma[:, None]).item()
        
        # Cross entropy (R package formula) with numerical stability
        R_sigma = self.R * self.sigma[:, None]
        # Clamp to avoid log(0) or division by zero
        O_clamped = torch.clamp(self.O, min=1e-8)
        E_clamped = torch.clamp(self.E, min=1e-8)
        ratio = (O_clamped + E_clamped) / E_clamped
        theta_log = self.theta.unsqueeze(0).expand(self.K, -1) * torch.log(ratio)
        _cross_entropy = torch.sum(R_sigma * (theta_log @ self.Phi)).item()
        
        # Store with normalization constant
        self.objective_kmeans.append((kmeans_error + _entropy + _cross_entropy) * norm_const)
        self.objective_kmeans_dist.append(kmeans_error * norm_const)
        self.objective_kmeans_entropy.append(_entropy * norm_const)
        self.objective_kmeans_cross.append(_cross_entropy * norm_const)

    def harmonize(self, iter_harmony=10, verbose=True):
        converged = False
        for i in range(1, iter_harmony + 1):
            if verbose:
                logger.info(f"Iteration {i} of {iter_harmony}")
            
            self.cluster()
            self.moe_correct_ridge()
            
            converged = self.check_convergence(1)
            if converged:
                if verbose:
                    logger.info(f"Converged after {i} iteration{'s' if i > 1 else ''}")
                break
                
        if verbose and not converged:
            logger.info("Stopped before convergence")

    def cluster(self):
        self.dist_mat = 2 * (1 - self.Y.T @ self.Z_cos)
        
        rounds = 0
        for i in range(self.max_iter_kmeans):
            # Update Y
            self.Y = self.Z_cos @ self.R.T
            self.Y = self.Y / torch.linalg.norm(self.Y, ord=2, dim=0)
            
            # Update distance matrix
            self.dist_mat = 2 * (1 - self.Y.T @ self.Z_cos)
            
            # Update R
            self.update_R()
            
            # Compute objective and check convergence
            self.compute_objective()
            
            if i > self.window_size:
                if self.check_convergence(0):
                    rounds = i + 1
                    break
            rounds = i + 1
            
        self.kmeans_rounds.append(rounds)
        self.objective_harmony.append(self.objective_kmeans[-1])

    def update_R(self):
        # Compute scaled distances
        self._scale_dist = -self.dist_mat / self.sigma[:, None]
        self._scale_dist = torch.exp(self._scale_dist)
        self._scale_dist = self._scale_dist / self._scale_dist.sum(dim=0)
        
        # Create shuffled update order
        update_order = torch.randperm(self.N, device=self.device)
        
        # Process in blocks
        n_blocks = int(np.ceil(1.0 / self.block_size))
        cells_per_block = int(self.N * self.block_size)
        
        # Permute matrices
        R_perm = self.R[:, update_order]
        scale_perm = self._scale_dist[:, update_order]
        Phi_perm = self.Phi[:, update_order]
        
        for blk in range(n_blocks):
            idx_min = blk * cells_per_block
            idx_max = self.N if blk == n_blocks - 1 else (blk + 1) * cells_per_block
            
            R_block = R_perm[:, idx_min:idx_max]
            scale_block = scale_perm[:, idx_min:idx_max]
            Phi_block = Phi_perm[:, idx_min:idx_max]
            
            # Remove cells from statistics
            self.E -= torch.outer(R_block.sum(dim=1), self.Pr_b)
            self.O -= R_block @ Phi_block.T
            
            # Recompute R for this block (R package formula) with numerical stability
            O_E_sum = self.O + self.E
            O_E_sum = torch.clamp(O_E_sum, min=1e-8)
            ratio = self.E / O_E_sum
            ratio = torch.clamp(ratio, min=1e-8, max=1.0)
            ratio_powered = harmony_pow_torch(ratio, self.theta)
            R_block_new = scale_block * (ratio_powered @ Phi_block)
            R_block_sum = R_block_new.sum(dim=0)
            R_block_sum = torch.clamp(R_block_sum, min=1e-8)
            R_block_new = R_block_new / R_block_sum
            
            # Put cells back
            self.E += torch.outer(R_block_new.sum(dim=1), self.Pr_b)
            self.O += R_block_new @ Phi_block.T
            
            R_perm[:, idx_min:idx_max] = R_block_new
        
        # Restore original order
        inverse_order = torch.argsort(update_order)
        self.R = R_perm[:, inverse_order]

    def check_convergence(self, i_type):
        if i_type == 0:
            if len(self.objective_kmeans) <= self.window_size + 1:
                return False
            
            w = self.window_size
            obj_old = sum(self.objective_kmeans[-w-1:-1])
            obj_new = sum(self.objective_kmeans[-w:])
            return abs(obj_old - obj_new) / abs(obj_old) < self.epsilon_kmeans
        
        if i_type == 1:
            if len(self.objective_harmony) < 2:
                return False
            
            obj_old = self.objective_harmony[-2]
            obj_new = self.objective_harmony[-1]
            return (obj_old - obj_new) / abs(obj_old) < self.epsilon_harmony
        
        return True

    def moe_correct_ridge(self):
        """Ridge regression correction for batch effects."""
        self._Z_corr = self.Z_orig.clone()
        
        for k in range(self.K):
            # Compute lambda if estimating
            if self.lambda_estimation:
                lamb_vec = find_lambda_torch(self.alpha, self.E[k, :], self.device)
            else:
                lamb_vec = self.lamb
            
            # Phi_Rk = Phi_moe scaled by R[k,:]
            Phi_Rk = self.Phi_moe * self.R[k, :]
            
            # Compute covariance
            cov_mat = Phi_Rk @ self.Phi_moe.T + torch.diag(lamb_vec)
            
            # Invert
            inv_cov = torch.linalg.inv(cov_mat)
            
            # Calculate R-scaled PCs
            Z_tmp = self.Z_orig * self.R[k, :]
            
            # Generate betas using the batch index
            W = inv_cov[:, 0:1] @ Z_tmp.sum(dim=1, keepdim=True).T
            
            for b in range(self.B):
                batch_sum = Z_tmp[:, self.batch_index[b]].sum(dim=1, keepdim=True)
                W = W + inv_cov[:, b+1:b+2] @ batch_sum.T
            
            W[0, :] = 0  # Do not remove intercept
            self._Z_corr = self._Z_corr - W.T @ Phi_Rk
        
        # Update Z_cos
        self.Z_cos = self._Z_corr / torch.linalg.norm(self._Z_corr, ord=2, dim=0)


def safe_entropy_torch(x):
    """Compute x * log(x), returning 0 where x is 0 or negative."""
    result = x * torch.log(x)
    result = torch.where(torch.isfinite(result), result, torch.zeros_like(result))
    return result


def harmony_pow_torch(A, T):
    """Element-wise power with different exponents per column."""
    result = torch.empty_like(A)
    for c in range(A.shape[1]):
        result[:, c] = torch.pow(A[:, c], T[c])
    return result


def find_lambda_torch(alpha, cluster_E, device):
    """Compute dynamic lambda based on cluster expected counts."""
    lamb = torch.zeros(len(cluster_E) + 1, dtype=torch.float32, device=device)
    lamb[1:] = cluster_E * alpha
    return lamb
